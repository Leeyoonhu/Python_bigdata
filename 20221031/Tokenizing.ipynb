{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73aaa881",
   "metadata": {},
   "source": [
    "# 영문, 한글 토크나이저를 구분해야함\n",
    "# 영문 토크나이저 대표적인건 nltk, spacy\n",
    "\n",
    "## 영어 토크나이징\n",
    "- NLTK(Natural Language Toolkit) 과 Spacy 가 대표적 임  \n",
    "## 1. NLTK  \n",
    "- 파이썬에서 영어 텍스트 전처리 작업을 하는 데 많이 쓰이는 라이브러리  \n",
    "- 50여개가 넘는 말뭉치(corpus) 리소스를 활용해 영어 텍스트를 분석할 수 있게 해줌  \n",
    "- 직관적인 함수 사용법으로 빠르게 텍스트 전처리를 할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc961a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc72546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk의 말뭉치 데이터를 다운받아야 함\n",
    "nltk.download('all-corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9884e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8584b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영문 기준으로 nltk 사용법 보기\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809abfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 문장\n",
    "sentence = \"Natural language processing (NLP) is a subfield of computer science, \\\n",
    "information engineering, and artificial intelligence concerned with the interactions \\\n",
    "between computers and human (natural) languages, in particular how to program computers \\\n",
    "to process and analyze large amounts of natural language data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7518a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# 영어 단어기준으로 자름\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85533baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 기준으로 자름\n",
    "paragraph = \"Natural language processing (NLP) is a subfield of computer science, \\\n",
    "information engineering, and artificial intelligence concerned with the interactions \\\n",
    "between computers and human (natural) languages, in particular how to program computers \\\n",
    "to process and analyze large amounts of natural language data. Challenges in natural \\\n",
    "language processing frequently involve speech recognition, natural language \\\n",
    "understanding, and natural language generation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0c6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.', 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1784d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정관사 불용어 사전\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ac1bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "# 그렇다면 얘가가진 불용어는 몇개나 될까\n",
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef013dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'computer', 'science', ',', 'information', 'engineering', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', '(', 'natural', ')', 'languages', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "# 불용어를 걸러내고 담을 것임\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c753b1",
   "metadata": {},
   "source": [
    "## 2. Spacy\n",
    "- 상업용 목적으로 만들어진 오픈소스 라이브러리  \n",
    "- 영어를 포함한 8개 국어에 대한 자연어 전처리 모듈을 제공  \n",
    "- 쉬운 설치 및 빠른 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b92ed",
   "metadata": {},
   "source": [
    "## 한글 토크나이징\n",
    "- 한글은 언어의 특성상 NLTK나 Spacy는 사용하기에 적합하지 않음  \n",
    "- 영어에 존재하지 않는 형태소 분석이나 음소 분리와 같은 내용은 다루기 어려움  \n",
    "- 한글 자연어 처리에 많이 사용되는 KoNLPy 에 대해 알아봄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc503db",
   "metadata": {},
   "source": [
    "## KoLNPy\n",
    "- 의미를 가지는 가장 작은 단위 = 형태소\n",
    "- 한글 자연어 처리를 위해 만들어진 오픈소스 라이브러리  \n",
    "- 국내에 이미 만들어져 사용되고 있는 여러 형태소 분석기를 사용할 수 있음  \n",
    "- 자바로 작성된 형태소 분석기를 사용하기 때문에 윈도우에서 KoNLPy를 설치하기 위해서는 Java(1.7 이상)가 필요  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48abdb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자바하고 파이썬 연결 인스톨\n",
    "# conda install -c conda-forge jpype1 -y\n",
    "# pip install konlpy\n",
    "\n",
    "import konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6bb2c",
   "metadata": {},
   "source": [
    "### 형태소 단위 토크나이징\n",
    "- KoNLPy에서는 여러 형태소 분석기를 제공  \n",
    "- 각 형태소 분석기는 클래스 형태로 되어 있고, 이를 객체로 생성한 후 메서드를 호출해서 토크나이징 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf6e4d",
   "metadata": {},
   "source": [
    "### 형태소 분석 및 품사 태깅\n",
    "- 형태소란 의미를 가지는 가장 작은 단위  \n",
    "- KoNLPy에 객체 형태로 포함되어 있는 형태소 분석기 목록  \n",
    "a. Hannanum  \n",
    "b. Kkma  \n",
    "c. Komoran  \n",
    "d. Mecab  \n",
    "e. Okt(Twitter)  \n",
    "- 위 객체들은 모두 동일하게 형태소 분석 기능을 제공  \n",
    "- 각기 성능이 조금씩 다름  \n",
    "- Mecab은 윈도우에서 실행 불가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbd59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 은 일본어도 가능\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7579fb8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m okt \u001b[38;5;241m=\u001b[39m \u001b[43mOkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m한글 자연어 처리는 재밌다 이제부터 열심히 해야지ㅎㅎㅎ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(okt\u001b[38;5;241m.\u001b[39mmorphs(text))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py40-env\\lib\\site-packages\\konlpy\\tag\\_okt.py:51\u001b[0m, in \u001b[0;36mOkt.__init__\u001b[1;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     oktJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.okt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m     OktInterfaceJavaClass \u001b[38;5;241m=\u001b[39m oktJavaPackage\u001b[38;5;241m.\u001b[39mOktInterface\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py40-env\\lib\\site-packages\\konlpy\\jvm.py:55\u001b[0m, in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     52\u001b[0m args \u001b[38;5;241m=\u001b[39m [javadir, os\u001b[38;5;241m.\u001b[39msep]\n\u001b[0;32m     53\u001b[0m classpath \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folder_suffix]\n\u001b[1;32m---> 55\u001b[0m jvmpath \u001b[38;5;241m=\u001b[39m jvmpath \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\\\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibjvm.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py40-env\\lib\\site-packages\\jpype\\_jvmfinder.py:74\u001b[0m, in \u001b[0;36mgetDefaultJVMPath\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     finder \u001b[38;5;241m=\u001b[39m LinuxJVMFinder()\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py40-env\\lib\\site-packages\\jpype\\_jvmfinder.py:212\u001b[0m, in \u001b[0;36mJVMFinder.get_jvm_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m                            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libfile))\n",
      "\u001b[1;31mJVMNotFoundException\u001b[0m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "text = \"한글 자연어 처리는 재밌다 이제부터 열심히 해야지ㅎㅎㅎ\"\n",
    "print(okt.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135893bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.phrases(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da721d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.pos(text, join=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma =Kkma()\n",
    "print(kkma.morphs(text))\n",
    "print(kkma.nouns(text))\n",
    "print(kkma.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ced96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = Hannanum()\n",
    "print(hannanum.morphs(text))\n",
    "print(hannanum.nouns(text))\n",
    "print(hannanum.pos(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
